{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f1402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def resample_patchemb(old_weight: torch.Tensor, new_patch_len: int):\n",
    "    \"\"\"\n",
    "    Flex-Resize 核心算法: 将 Embedding 权重从 Reference Length 重采样到 New Length。\n",
    "    \n",
    "    Args:\n",
    "        old_weight: 原始参考权重 [d_model, reference_len]\n",
    "        new_patch_len: 目标周期长度 (P_i)\n",
    "    Returns:\n",
    "        resampled_weight: 调整后的权重 [d_model, new_patch_len]\n",
    "    \"\"\"\n",
    "    # 维度检查与短路处理\n",
    "    assert old_weight.dim() == 2, \"输入张量应为2D (d_model, patch_size)\"\n",
    "    if old_weight.size(1) == new_patch_len:\n",
    "        return old_weight\n",
    "\n",
    "    # 1. 转置为 [reference_len, d_model] 以便矩阵计算\n",
    "    old = old_weight.T\n",
    "    old_len = old.size(0)\n",
    "    \n",
    "    # 2. 计算缩放因子 delta (对应论文中的 scaling factor)\n",
    "    # factor = P_new / P_ref\n",
    "    factor = new_patch_len / old_len\n",
    "    \n",
    "    # 定义辅助插值函数: 模拟矩阵 A 的线性变换\n",
    "    def resize_fn(x_tensor, new_shape):\n",
    "        # [L, L] -> [1, 1, L, L] -> interpolate -> [L, new_shape]\n",
    "        return F.interpolate(x_tensor.unsqueeze(0), size=new_shape, mode='linear', align_corners=False).squeeze(0)\n",
    "\n",
    "    # 3. 构造变换矩阵 A (通过对单位矩阵进行插值得到)\n",
    "    # basis_vectors: [old_len, old_len]\n",
    "    basis_vectors = torch.eye(old_len, dtype=torch.float32, device=old.device)\n",
    "    \n",
    "    # resize_mat 即为线性插值矩阵 A 的转置形式? \n",
    "    # resize 返回 [old_len, new_len]，这里做了 .T 变为 [new_len, old_len]\n",
    "    # 这里的实现逻辑是构造从 old 映射到 new 的算子\n",
    "    resize_mat = resize_fn(basis_vectors, new_patch_len).T \n",
    "    \n",
    "    # 4. 计算伪逆 (Moore-Penrose Pseudoinverse)\n",
    "    # 对应公式中的 (A)^+\n",
    "    # pinv([old_len, new_len]) -> [new_len, old_len]\n",
    "    resize_mat_pinv = torch.linalg.pinv(resize_mat.T)\n",
    "    \n",
    "    # 5. 应用变换与缩放\n",
    "    # new_weight = (A)^+ @ old_weight * sqrt(factor)\n",
    "    # [new_len, old_len] @ [old_len, d_model] -> [new_len, d_model]\n",
    "    resampled_kernels = resize_mat_pinv @ old * math.sqrt(factor)\n",
    "\n",
    "    # 6. 转置回 [d_model, new_len] 以适配 nn.Linear\n",
    "    return resampled_kernels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75c082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexResizeProjector(nn.Module):\n",
    "    def __init__(self, d_model, reference_len=48):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.reference_len = reference_len\n",
    "        \n",
    "        # 初始化共享的参考权重 (Reference Weights)\n",
    "        # 对应源码中的 self.embedding = nn.Linear(self.target_patch_len, d_model)\n",
    "        # shape: [d_model, reference_len] (注意 nn.Linear 存储权重是转置的)\n",
    "        self.reference_embedding = nn.Linear(reference_len, d_model)\n",
    "\n",
    "    def forward(self, x_patches, period):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_patches: 输入切片 [Batch, Num_Patches, ..., Period]\n",
    "            period: 当前输入的物理周期长度 (int)\n",
    "        Returns:\n",
    "            projected: 投影后的 Token [Batch, Num_Patches, ..., D_model]\n",
    "        \"\"\"\n",
    "        # 1. 获取适应当前周期的权重\n",
    "        # 如果 period == reference_len，resample_patchemb 会直接返回原权重\n",
    "        # 注意：这里我们操作的是 weight.data，但在 forward 中为了保持梯度流，\n",
    "        # 我们应该直接对 weight 变量进行计算（resample_patchemb内部全是可导的torch操作）\n",
    "        current_weight = resample_patchemb(self.reference_embedding.weight, period)\n",
    "        \n",
    "        # 2. 执行线性投影\n",
    "        # x @ W.T + b\n",
    "        # 输入 x: [..., Period]\n",
    "        # 权重 W: [D_model, Period] -> W.T: [Period, D_model]\n",
    "        # 输出: [..., D_model]\n",
    "        \n",
    "        # 使用 F.linear 进行手动投影，因为我们动态生成了 weight\n",
    "        # bias 可以共享，也可以缩放，LightGTS 源码中仅对 weight 做了 resample\n",
    "        out = F.linear(x_patches, current_weight, self.reference_embedding.bias)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d4f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Period Input (24) -> Output: torch.Size([32, 10, 256])\n",
      "Long Period Input (96) -> Output: torch.Size([32, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "d_model = 256\n",
    "ref_len = 48\n",
    "projector = FlexResizeProjector(d_model, ref_len)\n",
    "\n",
    "# 场景 1: 输入周期为 24 (短周期)\n",
    "x_short = torch.randn(32, 10, 24) # [Batch, Patch_Num, Period]\n",
    "out_short = projector(x_short, period=24)\n",
    "print(f\"Short Period Input (24) -> Output: {out_short.shape}\") # Expect: [32, 10, 256]\n",
    "\n",
    "# 场景 2: 输入周期为 96 (长周期)\n",
    "x_long = torch.randn(32, 2, 96)\n",
    "out_long = projector(x_long, period=96)\n",
    "print(f\"Long Period Input (96) -> Output: {out_long.shape}\")   # Expect: [32, 2, 256]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
